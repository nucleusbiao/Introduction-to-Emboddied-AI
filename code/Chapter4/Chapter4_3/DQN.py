"""
Improved DQN for CartPole with better convergence
"""

import random
import math
import collections
from dataclasses import dataclass
from typing import Deque, Tuple, List
import time

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

import gymnasium as gym

# ============= Improved Hyperparameters =============
@dataclass
class Config:
    env_name: str = "CartPole-v1"
    seed: int = 42
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # training
    total_episodes: int = 800  # å¢åŠ è®­ç»ƒè½®æ¬¡
    max_steps_per_episode: int = 500
    batch_size: int = 32  # å‡å°batch size
    gamma: float = 0.99
    lr: float = 1e-3  # é€‚å½“çš„å­¦ä¹ ç‡

    # exploration
    eps_start: float = 1.0
    eps_end: float = 0.01
    eps_decay: int = 300  # æ›´é•¿çš„æ¢ç´¢è¡°å‡

    # replay buffer
    replay_size: int = 10000
    min_replay_size: int = 1000

    # target network update
    target_update_freq: int = 100  # æŒ‰æ­¥æ•°æ›´æ–°ï¼Œè€Œä¸æ˜¯æŒ‰episode

    # logging
    eval_every: int = 20

cfg = Config()

# ============= Simple but Effective Network =============
class QNetwork(nn.Module):
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        # ä½¿ç”¨æ›´ç®€å•çš„ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)

class ReplayBuffer:
    def __init__(self, capacity: int):
        self.buffer: Deque[Tuple] = collections.deque(maxlen=capacity)

    def push(self, *transition):
        self.buffer.append(transition)

    def sample(self, batch_size: int):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)

# ============= Improved Agent =============
class DQNAgent:
    def __init__(self, state_dim: int, action_dim: int, cfg: Config):
        self.device = cfg.device
        self.action_dim = action_dim
        self.cfg = cfg

        # ç½‘ç»œ
        self.q_net = QNetwork(state_dim, action_dim).to(self.device)
        self.target_net = QNetwork(state_dim, action_dim).to(self.device)
        self.target_net.load_state_dict(self.q_net.state_dict())

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=cfg.lr)
        self.replay_buffer = ReplayBuffer(cfg.replay_size)

        # è®­ç»ƒè®¡æ•°å™¨
        self.steps_done = 0

    def get_epsilon(self):
        """çº¿æ€§è¡°å‡çš„epsilon"""
        eps = self.cfg.eps_end + (self.cfg.eps_start - self.cfg.eps_end) * \
              math.exp(-1. * self.steps_done / self.cfg.eps_decay)
        return eps

    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """åŠ¨ä½œé€‰æ‹©"""
        if training and random.random() < self.get_epsilon():
            return random.randrange(self.action_dim)
        else:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            with torch.no_grad():
                q_values = self.q_net(state_tensor)
            return q_values.argmax().item()

    def store_transition(self, state, action, reward, next_state, done):
        self.replay_buffer.push(state, action, reward, next_state, done)

    def update(self):
        """è®­ç»ƒæ›´æ–°"""
        if len(self.replay_buffer) < self.cfg.min_replay_size:
            return None

        # é‡‡æ ·
        transitions = self.replay_buffer.sample(self.cfg.batch_size)
        batch = list(zip(*transitions))

        states = torch.FloatTensor(np.array(batch[0])).to(self.device)
        actions = torch.LongTensor(np.array(batch[1])).unsqueeze(1).to(self.device)
        rewards = torch.FloatTensor(np.array(batch[2])).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(np.array(batch[3])).to(self.device)
        dones = torch.BoolTensor(np.array(batch[4])).unsqueeze(1).to(self.device)

        # å½“å‰Qå€¼
        current_q_values = self.q_net(states).gather(1, actions)

        # ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q_values = rewards + (self.cfg.gamma * next_q_values * ~dones)

        # æŸå¤±è®¡ç®—
        loss = nn.functional.mse_loss(current_q_values, target_q_values)

        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
        self.optimizer.step()

        self.steps_done += 1

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.steps_done % self.cfg.target_update_freq == 0:
            self.target_net.load_state_dict(self.q_net.state_dict())

        return loss.item()

# ============= Training with Better Monitoring =============
def evaluate_agent(env, agent, n_episodes=5):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    total_rewards = []
    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False

        while not done:
            action = agent.select_action(state, training=False)
            next_state, reward, done, truncated, _ = env.step(action)
            done = done or truncated
            state = next_state
            episode_reward += reward

        total_rewards.append(episode_reward)

    return np.mean(total_rewards), np.std(total_rewards)

def train():
    # ç¯å¢ƒè®¾ç½®
    env = gym.make(cfg.env_name)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(cfg.seed)
    np.random.seed(cfg.seed)
    random.seed(cfg.seed)
    if hasattr(env, 'reset'):
        env.reset(seed=cfg.seed)

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    print(f"Training on {cfg.device}")
    print(f"State dimension: {state_dim}, Action dimension: {action_dim}")

    agent = DQNAgent(state_dim, action_dim, cfg)

    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = []
    moving_averages = []
    losses = []
    best_mean_reward = 0

    print("\nStarting training...")
    print("Phase 1: Random exploration (first 1000 steps)")

    # åˆå§‹æ¢ç´¢é˜¶æ®µ
    state, _ = env.reset()
    for step in range(cfg.min_replay_size):
        action = env.action_space.sample()  # å®Œå…¨éšæœºæ¢ç´¢
        next_state, reward, done, truncated, _ = env.step(action)
        agent.store_transition(state, action, reward, next_state, done)

        if done or truncated:
            state, _ = env.reset()
        else:
            state = next_state

        if step % 200 == 0:
            print(f"  Collected {step}/{cfg.min_replay_size} random transitions")

    print("Phase 2: Start training with experience replay")

    # ä¸»è®­ç»ƒå¾ªç¯
    for episode in range(1, cfg.total_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        episode_loss = 0
        update_count = 0

        for step in range(cfg.max_steps_per_episode):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, truncated, _ = env.step(action)
            done = done or truncated

            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, reward, next_state, done)

            # è®­ç»ƒ
            loss = agent.update()
            if loss is not None:
                episode_loss += loss
                update_count += 1

            state = next_state
            episode_reward += reward

            if done:
                break

        # è®°å½•ç»Ÿè®¡
        episode_rewards.append(episode_reward)

        if update_count > 0:
            avg_loss = episode_loss / update_count
            losses.append(avg_loss)
        else:
            losses.append(0)

        # è®¡ç®—ç§»åŠ¨å¹³å‡
        if len(episode_rewards) >= 10:
            moving_avg = np.mean(episode_rewards[-10:])
            moving_averages.append(moving_avg)
        else:
            moving_avg = np.mean(episode_rewards)
            moving_averages.append(moving_avg)

        # å®šæœŸè¯„ä¼°å’Œä¿å­˜
        if episode % cfg.eval_every == 0:
            eval_mean, eval_std = evaluate_agent(env, agent)
            current_epsilon = agent.get_epsilon()

            print(f"Episode {episode:4d} | "
                  f"Reward: {episode_reward:5.1f} | "
                  f"Avg10: {moving_avg:6.2f} | "
                  f"Eval: {eval_mean:5.1f} Â± {eval_std:3.1f} | "
                  f"Epsilon: {current_epsilon:.3f} | "
                  f"Loss: {avg_loss if update_count > 0 else 0:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_mean > best_mean_reward:
                best_mean_reward = eval_mean
                torch.save(agent.q_net.state_dict(), "dqn_cartpole_best.pth")
                print(f"  ğŸ’¾ New best model saved! (Score: {eval_mean:.1f})")

            # æå‰åœæ­¢æ¡ä»¶
            if eval_mean >= 495 and episode >= 200:
                print(f"\nğŸ‰ Training completed! Agent achieved near-perfect performance.")
                break

        # æ£€æŸ¥æ˜¯å¦å­¦ä¹ å¤ªæ…¢
        if episode == 200 and moving_avg < 100:
            print("âš ï¸  Training seems slow. Consider adjusting hyperparameters.")
        elif episode == 400 and moving_avg < 300:
            print("âš ï¸  Training progress is suboptimal. You might want to restart with different parameters.")

    env.close()

    # æœ€ç»ˆè¯„ä¼°
    print("\nFinal evaluation...")
    final_env = gym.make(cfg.env_name)
    final_mean, final_std = evaluate_agent(final_env, agent, n_episodes=10)
    final_env.close()

    print(f"Final performance: {final_mean:.1f} Â± {final_std:.1f}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        import matplotlib.pyplot as plt

        plt.figure(figsize=(12, 10))

        # å¥–åŠ±æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(episode_rewards, alpha=0.3, color='blue', label='Episode Reward')
        plt.plot(moving_averages, color='red', linewidth=2, label='Moving Average (10)')
        plt.axhline(y=475, color='green', linestyle='--', label='Target (475)')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.title('Training Progress')
        plt.legend()
        plt.grid(True)

        # æŸå¤±æ›²çº¿
        plt.subplot(2, 2, 2)
        plt.plot(losses)
        plt.xlabel('Episode')
        plt.ylabel('Loss')
        plt.title('Training Loss')
        plt.grid(True)

        # epsilonè¡°å‡
        plt.subplot(2, 2, 3)
        epsilons = [agent.get_epsilon() for _ in range(len(episode_rewards))]
        plt.plot(epsilons)
        plt.xlabel('Episode')
        plt.ylabel('Epsilon')
        plt.title('Exploration Rate Decay')
        plt.grid(True)

        # æœ€ç»ˆæ€§èƒ½åˆ†å¸ƒ
        plt.subplot(2, 2, 4)
        final_rewards = []
        test_env = gym.make(cfg.env_name)
        for _ in range(20):
            state, _ = test_env.reset()
            total_reward = 0
            done = False
            while not done:
                action = agent.select_action(state, training=False)
                state, reward, done, truncated, _ = test_env.step(action)
                done = done or truncated
                total_reward += reward
            final_rewards.append(total_reward)
        test_env.close()

        plt.hist(final_rewards, bins=20, alpha=0.7, edgecolor='black')
        plt.axvline(np.mean(final_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(final_rewards):.1f}')
        plt.xlabel('Final Reward')
        plt.ylabel('Frequency')
        plt.title('Final Performance Distribution')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('training_analysis.png', dpi=150, bbox_inches='tight')
        print("Saved training_analysis.png")

    except ImportError:
        print("Matplotlib not available, skipping plots")

    return agent, episode_rewards, moving_averages

def demonstrate_agent(agent, num_episodes=3, render=True):
    """å±•ç¤ºè®­ç»ƒå¥½çš„æ™ºèƒ½ä½“"""
    print(f"\n{'='*60}")
    print("DEMONSTRATING TRAINED AGENT")
    print(f"{'='*60}")

    if render:
        try:
            env = gym.make(cfg.env_name, render_mode='human')
        except:
            env = gym.make(cfg.env_name)
            render = False
            print("Note: Visualization not available")
    else:
        env = gym.make(cfg.env_name)

    rewards = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        done = False

        print(f"\nEpisode {episode + 1}: ", end="")

        while not done and steps < 1000:  # å¢åŠ æœ€å¤§æ­¥æ•°é™åˆ¶
            action = agent.select_action(state, training=False)
            next_state, reward, done, truncated, _ = env.step(action)
            done = done or truncated

            state = next_state
            total_reward += reward
            steps += 1

            if render:
                env.render()
                time.sleep(0.02)  # å‡æ…¢é€Ÿåº¦ä¾¿äºè§‚å¯Ÿ

            if done:
                break

        rewards.append(total_reward)
        print(f"Steps: {steps}, Total Reward: {total_reward}")

        # æ€§èƒ½è¯„ä»·
        if total_reward >= 495:
            print("  ğŸ‰ Perfect! Agent maintains perfect balance.")
        elif total_reward >= 450:
            print("  ğŸ‘ Excellent! Very stable control.")
        elif total_reward >= 400:
            print("  âœ… Good! Reliable performance.")
        elif total_reward >= 300:
            print("  ğŸ”¶ Acceptable. Some room for improvement.")
        else:
            print("  ğŸ”» Needs more training.")

    env.close()

    # æ€»ç»“
    avg_reward = np.mean(rewards)
    print(f"\nSummary: Average reward over {num_episodes} episodes: {avg_reward:.1f}")

    if avg_reward >= 480:
        print("ğŸŠ SUCCESS! The agent has successfully learned to control CartPole!")
    elif avg_reward >= 400:
        print("ğŸ‘ Good results! The agent performs well.")
    else:
        print("ğŸ’¡ Consider training for more episodes.")

if __name__ == '__main__':
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--demo":
        # æ¼”ç¤ºæ¨¡å¼
        print("Loading pre-trained model for demonstration...")
        env = gym.make(cfg.env_name)
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.n
        env.close()

        agent = DQNAgent(state_dim, action_dim, cfg)

        try:
            agent.q_net.load_state_dict(torch.load("dqn_cartpole_best.pth", map_location=cfg.device))
            print("Model loaded successfully!")
            demonstrate_agent(agent, num_episodes=5, render=True)
        except FileNotFoundError:
            print("No trained model found. Please run training first.")
    else:
        # è®­ç»ƒæ¨¡å¼
        print("CartPole DQN Training")
        print("=" * 50)
        agent, rewards, moving_avgs = train()

        # è®­ç»ƒåæ¼”ç¤º
        print("\n" + "=" * 60)
        print("POST-TRAINING DEMONSTRATION")
        print("=" * 60)
        demonstrate_agent(agent, num_episodes=5, render=True)